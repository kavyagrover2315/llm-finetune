import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from peft import PeftModel, PeftConfig
import torch

# === Load fine-tuned model ===
MODEL_PATH = "./copilot-lora-model"

st.set_page_config(page_title="Mini Copilot", layout="wide")

@st.cache_resource
def load_model():
    base_model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        device_map="auto",
        torch_dtype=torch.float16,
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    tokenizer.pad_token = tokenizer.eos_token
    return base_model.eval(), tokenizer

model, tokenizer = load_model()

# === Generate response ===
def generate_code(prompt):
    input_text = f"<|user|>\n{prompt}\n<|assistant|>\n"
    inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=150,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return decoded.split("<|assistant|>")[-1].strip()

# === Streamlit UI ===
st.title("ðŸ§  Mini Copilot: AI Code Assistant")
st.markdown("Built with StarCoder + LoRA")

code_prompt = st.text_area("ðŸ’¬ Enter your prompt", placeholder="e.g., Write a Python class for a calculator...")

if st.button("ðŸš€ Generate Code"):
    if code_prompt.strip() == "":
        st.warning("Please enter a prompt.")
    else:
        with st.spinner("Thinking like Copilot..."):
            result = generate_code(code_prompt)
            st.code(result, language="python")

